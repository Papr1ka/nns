{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Наши разработки из 3-ей работы))\n",
    "from nn import Nums\n",
    "from nn import NN, Layer\n",
    "from nn.optimizers import AdamOptimizer\n",
    "\n",
    "type Nums = np.ndarray[np.number]\n",
    "type NumsToNums = Callable[[Nums], Nums]\n",
    "type NumsNumsToNums = Callable[[Nums, Nums], Nums]\n",
    "\n",
    "np.set_printoptions(formatter={\"float\": lambda x: \"{0:0.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет, one-hot-encoding для меток классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000, 10))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "\n",
    "train_x = train_x / 255\n",
    "test_x = test_x / 255\n",
    "\n",
    "train_x.resize(60000, 784)\n",
    "test_x.resize(10000, 784)\n",
    "\n",
    "encoder = sklearn.preprocessing.OneHotEncoder()\n",
    "train_y = encoder.fit_transform(train_y[..., None]).toarray()\n",
    "test_y = encoder.fit_transform(test_y[..., None]).toarray()\n",
    "\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.121, 0.415, 0.496],\n",
       "       [0.631, 0.331, 0.630],\n",
       "       [0.628, 0.713, 0.791]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.random.rand(3, 3)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x: Nums, core: Nums, strides: Tuple[int, int]=(1, 1), padding: Tuple[int, int]=(0, 0)) -> Nums:\n",
    "    \"\"\"2д свёртка\n",
    "\n",
    "    Args:\n",
    "        x (Nums): Входной массив, 1d\n",
    "        x_shape (Tuple[int, int]): Размерность входного массива 2d (Как интерпретировать, для mnist - (28, 28))\n",
    "        core (Nums): Ядро свёртки\n",
    "        strides (Tuple[int, int], optional): Шаги свёртки по x и y. Defaults to (1, 1).\n",
    "        padding (Tuple[int, int], optional): Отступы по x и y. Defaults to (1, 1).\n",
    "\n",
    "    Returns:\n",
    "        Nums: Результат свёртки\n",
    "    \"\"\"\n",
    "    if padding != (0, 0):\n",
    "        new_shape = (x.shape[0] + padding[0] * 2, x.shape[1] + padding[1] * 2)\n",
    "        new_x = np.zeros(new_shape)  # todo: make more efficient\n",
    "        new_x[padding[0]:-padding[0], padding[1]:-padding[1]] = x\n",
    "        x = new_x\n",
    "    \n",
    "    output_shape = (np.subtract(x.shape, core.shape) + strides) / strides\n",
    "    output_shape = tuple(np.floor(output_shape, casting='unsafe', dtype=int))\n",
    "    views_shape = output_shape + core.shape\n",
    "    \n",
    "    row_strides, col_strides = x.strides\n",
    "    \n",
    "    views = np.lib.stride_tricks.as_strided(\n",
    "        x, shape=views_shape, strides=(row_strides * strides[0], col_strides * strides[1], row_strides, col_strides)\n",
    "        )\n",
    "    return np.einsum('ijkl, kl -> ij', views, core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_x)):\n",
    "    y = conv2d(train_x[i].reshape((28, 28)), w, strides=(1, 1), padding=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[243], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m      2\u001b[0m     [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      3\u001b[0m     [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      4\u001b[0m     [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      5\u001b[0m     [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      6\u001b[0m     [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m      8\u001b[0m w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m      9\u001b[0m     [\n\u001b[1;32m     10\u001b[0m         [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     ]\n\u001b[1;32m     19\u001b[0m ])\n\u001b[0;32m---> 21\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m r\n",
      "Cell \u001b[0;32mIn[242], line 20\u001b[0m, in \u001b[0;36mconv2d\u001b[0;34m(x, core, strides, padding)\u001b[0m\n\u001b[1;32m     17\u001b[0m     new_x[padding[\u001b[38;5;241m0\u001b[39m]:\u001b[38;5;241m-\u001b[39mpadding[\u001b[38;5;241m0\u001b[39m], padding[\u001b[38;5;241m1\u001b[39m]:\u001b[38;5;241m-\u001b[39mpadding[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m new_x\n\u001b[0;32m---> 20\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m (\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m strides) \u001b[38;5;241m/\u001b[39m strides\n\u001b[1;32m     21\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(np\u001b[38;5;241m.\u001b[39mfloor(output_shape, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m))\n\u001b[1;32m     22\u001b[0m views_shape \u001b[38;5;241m=\u001b[39m output_shape \u001b[38;5;241m+\u001b[39m core\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (3,) "
     ]
    }
   ],
   "source": [
    "a = np.array([\n",
    "    [0, 1, 2, 1, 0],\n",
    "    [4, 1, 0, 1, 0],\n",
    "    [2, 0, 1, 1, 1],\n",
    "    [1, 2, 3, 1, 0],\n",
    "    [0, 4, 3, 2, 0]])\n",
    "\n",
    "w = np.array([\n",
    "    [\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 1],\n",
    "        [2, 1, 0]\n",
    "    ],\n",
    "    [\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 1],\n",
    "        [2, 1, 0]\n",
    "    ]\n",
    "])\n",
    "\n",
    "r = conv2d(a, w, strides=(2, 2))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(x: Nums, window: Tuple[int, int], strides: Tuple[int, int], func=np.max) -> Nums:\n",
    "    output_shape = (np.subtract(x.shape, window) + strides) / strides\n",
    "    output_shape = tuple(np.floor(output_shape, casting='unsafe', dtype=int))\n",
    "    \n",
    "    views_shape = output_shape + window\n",
    "    \n",
    "    row_strides, col_strides = x.strides\n",
    "    \n",
    "    views = np.lib.stride_tricks.as_strided(\n",
    "        x, shape=views_shape, strides=(row_strides * strides[0], col_strides * strides[1], row_strides, col_strides)\n",
    "    )\n",
    "    return views\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 2],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = pooling(np.array(\n",
    "    [\n",
    "        [0, 1, 2, 1],\n",
    "        [4, 1, 0, 1],\n",
    "        [2, 0, 1, 1],\n",
    "        [1, 2, 3, 1]\n",
    "    ]), window=(2, 2), strides=(2, 2))\n",
    "np.max(views, axis=(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable\n",
    "\n",
    "from .base import Nums, NumsToNums\n",
    "from .base import np\n",
    "from .optimizers import OptimizerBase\n",
    "\n",
    "\n",
    "class ConvLayer:\n",
    "    \"\"\"\n",
    "    Реализует свёрточный слой\n",
    "\n",
    "    input_size: int - количество входных нейронов\n",
    "    output_size: int - количество выходных нейронов\n",
    "    weights: np.ndarray[np.number] - веса слоя\n",
    "    bias: np.ndarray[np.number] - смещения\n",
    "\n",
    "    self.activation_function: NumsToNums - функция активации слоя\n",
    "    self.activation_function_derivation: NumsToNums - производная функции активации слоя\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size: Tuple[int, int],\n",
    "        strides: Tuple[int, int],\n",
    "        padding: Tuple[int, int],\n",
    "        pooling_function: NumsToNums,\n",
    "        activation_function: Tuple[\n",
    "            NumsToNums,\n",
    "            NumsToNums,\n",
    "        ],\n",
    "        weights_initialize_function: Callable[[Tuple[int, int]], Nums] = None,\n",
    "    ) -> None:\n",
    "        if weights_initialize_function is None:\n",
    "            weights_initialize_function = np.random.random\n",
    "\n",
    "        self.weights = weights_initialize_function(window_size)\n",
    "        self.bias = weights_initialize_function(window_size)\n",
    "        self.activation_function = activation_function[0]\n",
    "        self.activation_function_derivation = activation_function[1]\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.pooling_function = pooling_function\n",
    "\n",
    "        # Значения x и t, фиксируемые при forward, нужны для вычисления ошибки\n",
    "        self._xt = ()\n",
    "        \"\"\"\n",
    "        Контейнеры для хранения вычисленных ошибок для весов и смещений\n",
    "        Необходимы для того, чтобы можно было обучать батчами\n",
    "        \n",
    "        В батче, для каждого примера необходимо вызвать forward и backward.\n",
    "        В конце батча, для модификации весов, необходимо вызвать update\n",
    "        _errors_log будет очищен\n",
    "        \"\"\"\n",
    "        self._errors_log = ([], [])\n",
    "\n",
    "    def forward(self, x: Nums) -> Nums:\n",
    "        t = conv2d(x, self.weights, self.strides, self.padding)\n",
    "        h = self.activation_function(t)\n",
    "        p = pooling(h, (2, 2), (1, 1), self.pooling_function)\n",
    "        \n",
    "        self._xt = (x, t)\n",
    "        return h\n",
    "\n",
    "    def backward(self, error: Nums) -> Nums:\n",
    "        \"\"\"\n",
    "        Обратное распространение ошибки\n",
    "        Запоминает производные ошибки, но не изменяет параметры модели\n",
    "        Для изменения нужно вызвать update\n",
    "        \"\"\"\n",
    "        x, t = self._xt\n",
    "        self._xt = ()\n",
    "\n",
    "        de_dT = error * self.activation_function_derivation(t)\n",
    "        de_dW = x.T @ de_dT\n",
    "        de_dB = de_dT\n",
    "        de_dX = de_dT @ self.weights.T\n",
    "        self._errors_log[0].append(de_dW)\n",
    "        self._errors_log[1].append(de_dB)\n",
    "        return de_dX\n",
    "\n",
    "    def update(self, learning_rate: float):\n",
    "        \"\"\"\n",
    "        Модифицирует веса модели\n",
    "        learning_rate: float, [0, 1] - на сколько сильно модель будет реагировать на ошибку\n",
    "        \"\"\"\n",
    "        de_dWs, de_dBs = self._errors_log\n",
    "        de_dW = np.sum(de_dWs, axis=0)\n",
    "        de_dB = np.sum(de_dBs, axis=0)\n",
    "\n",
    "        self.optimizer.step(self.weights, self.bias, learning_rate, de_dW, de_dB)\n",
    "\n",
    "        self._errors_log = ([], [])\n",
    "\n",
    "    def set_optimizer(self, optimizer: OptimizerBase):\n",
    "        optimizer.init_params(self.weights, self.bias)\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"<LinearLayer, {self.input_size}x{self.output_size}>\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
